{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_2.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_len = 8\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 #The embedding dimension for the words\n",
    "d_ff = 4*d_model #The number of neurons for the linear layers in the projection layer\n",
    "heads = 32\n",
    "dropout = 0.1\n",
    "n_enc = 10 #This is the number of encoders we will use for the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "  def __init__(self , vocab_size: int , d_model: int):\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.d_model = d_model\n",
    "    self.embedding = nn.Embedding(vocab_size , d_model)\n",
    "\n",
    "  def forward(self , x):\n",
    "    return self.embedding(x) * math.sqrt(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoMaskAttention(nn.Module):\n",
    "  def __init__(self , heads: int , d_model: int , dropout: float):\n",
    "    super().__init__()\n",
    "    self.heads = heads\n",
    "    self.d_model = d_model\n",
    "    assert d_model % heads == 0 , \"d_model is not divisible by the number of heads\"\n",
    "    #Note that the input being passed now has the shape (batch_size , seq_len , d_model)\n",
    "    self.d_k = d_model//heads\n",
    "    self.w_q = nn.Linear(d_model , d_model , bias=False)\n",
    "    self.w_k = nn.Linear(d_model , d_model , bias=False)\n",
    "    self.w_v = nn.Linear(d_model , d_model , bias=False)\n",
    "\n",
    "    self.w_o = nn.Linear(d_model , d_model , bias=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  @staticmethod\n",
    "  def attention(self , q , k , v): \n",
    "    attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return (attention_scores @ v) , attention_scores\n",
    "\n",
    "  def forward(self , q , k , v):\n",
    "    q = self.w_q(q)\n",
    "    k = self.w_k(k)\n",
    "    v = self.w_v(v)\n",
    "\n",
    "    q = q.view(q.shape[0] , q.shape[1] , self.heads , self.d_k).transpose(1,2) #The shape becomes (batch_size , heads , seq_len , d_k)\n",
    "    k = k.view(k.shape[0] , k.shape[1] , self.heads , self.d_k).transpose(1,2)\n",
    "    v = v.view(v.shape[0] , v.shape[1] , self.heads , self.d_k).transpose(1,2)\n",
    "\n",
    "    x , self.attention_scores = NoMaskAttention.attention(self , q , k , v)\n",
    "\n",
    "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "    return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masked_Attention(nn.Module):\n",
    "  def __init__(self , heads: int , d_model: int , dropout: float):\n",
    "    super().__init__()\n",
    "    self.heads = heads\n",
    "    self.d_model = d_model\n",
    "    assert d_model % heads == 0 , \"d_model is not divisible by the number of heads\"\n",
    "    #Note that the input being passed now has the shape (batch_size , seq_len , d_model)\n",
    "    self.d_k = d_model//heads\n",
    "    self.w_q = nn.Linear(d_model , d_model , bias=False)\n",
    "    self.w_k = nn.Linear(d_model , d_model , bias=False)\n",
    "    self.w_v = nn.Linear(d_model , d_model , bias=False)\n",
    "\n",
    "    self.w_o = nn.Linear(d_model , d_model , bias=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  @staticmethod\n",
    "  def attention(self , q , k , v): #add the mask back to this\n",
    "    attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    decoder_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    attention_scores.masked_fill_(decoder_mask, -1e9)\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return (attention_scores @ v) , attention_scores\n",
    "\n",
    "  def forward(self , q , k , v):\n",
    "    q = self.w_q(q)\n",
    "    k = self.w_k(k)\n",
    "    v = self.w_v(v)\n",
    "\n",
    "    q = q.view(q.shape[0] , q.shape[1] , self.heads , self.d_k).transpose(1,2) #The shape becomes (batch_size , heads , seq_len , d_k)\n",
    "    k = k.view(k.shape[0] , k.shape[1] , self.heads , self.d_k).transpose(1,2)\n",
    "    v = v.view(v.shape[0] , v.shape[1] , self.heads , self.d_k).transpose(1,2)\n",
    "\n",
    "    x , self.attention_scores = Masked_Attention.attention(self , q , k , v)\n",
    "\n",
    "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "    return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self , d_model: int , d_ff: int , dropout: float):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_ff = d_ff\n",
    "    self.relu = nn.ReLU()\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.linear1 = nn.Linear(d_model , d_ff)\n",
    "    self.linear2 = nn.Linear(d_ff , d_model)\n",
    "\n",
    "  def forward(self ,x ):\n",
    "    x = self.linear1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "  def __init__(self , d_model: int , vocab_size: int):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.vocab_size = vocab_size\n",
    "    self.linear = nn.Linear(d_model , vocab_size)\n",
    "\n",
    "  def forward(self , x):\n",
    "    return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self , InputEmbedding , PositionalEncoding , NoMaskAttention , FeedForward , d_model: int , dropout: float):\n",
    "    super().__init__()\n",
    "    self.input_embedding_layer = InputEmbedding(vocab_size , d_model)\n",
    "    self.positional_encoding_layer = PositionalEncoding(d_model , seq_len , dropout=0.1)\n",
    "    self.attention = NoMaskAttention(heads=8 , d_model = 512 , dropout=0.1)\n",
    "    self.ffwd = FeedForward(d_model , d_ff , dropout=0.1)\n",
    "    self.layernorm = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self , x):\n",
    "    x = self.input_embedding_layer(x)\n",
    "    x = self.positional_encoding_layer(x)\n",
    "    x = x + self.dropout(self.attention(self.layernorm(x) , self.layernorm(x) , self.layernorm(x)))\n",
    "    x = x + self.dropout(self.attention(self.layernorm(x) , self.layernorm(x) , self.layernorm(x))) #AN IDEA TO TRY OUT WITH 2 ATTENTIONS IN ENCODER\n",
    "    x = x + self.dropout(self.ffwd(self.layernorm(x)))\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self , n_enc: int , Encoder , InputEmbedding , PositionalEncoding , NoMaskAttention , FeedForward , d_model: int , dropout: float):\n",
    "    super().__init__()\n",
    "    self.n_enc = n_enc\n",
    "    self.encoder = nn.ModuleList([Encoder(InputEmbedding , PositionalEncoding , NoMaskAttention , FeedForward , d_model , dropout) for _ in range(n_enc)])\n",
    "\n",
    "  def forward(self , x):\n",
    "    for encoder in self.encoder:\n",
    "      return encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1 , 25 , (batch_size , seq_len) , dtype=torch.int32)\n",
    "transformer_encoder_block = EncoderBlock(n_enc , Encoder , InputEmbedding , PositionalEncoding , NoMaskAttention , FeedForward , d_model=512 , dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder portion\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self , InputEmbedding , PositionalEncoding , Masked_Attention , FeedForward ,ProjectionLayer,  d_model: int , dropout: float):\n",
    "    super().__init__()\n",
    "    self.input_embedding_layer = InputEmbedding(vocab_size , d_model)\n",
    "    self.positional_encoding_layer = PositionalEncoding(d_model , seq_len , dropout=0.1)\n",
    "    self.attention = Masked_Attention(heads=8 , d_model=512 , dropout=0.1)\n",
    "    self.ffwd = FeedForward(d_model , d_ff , dropout=0.1)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.layernorm = nn.LayerNorm(d_model)\n",
    "    self.projection_layer = ProjectionLayer(d_model , vocab_size)\n",
    "\n",
    "  def forward(self , x , encoder_output):\n",
    "    x = self.input_embedding_layer(x)\n",
    "    x = self.positional_encoding_layer(x)\n",
    "    x = x + self.dropout(self.attention(self.layernorm(x) , self.layernorm(x) , self.layernorm(x)))\n",
    "    x = x + self.dropout(self.attention(self.layernorm(x) , self.layernorm(encoder_output) , self.layernorm(encoder_output)))\n",
    "    x = x + self.dropout(self.ffwd(self.layernorm(x)))\n",
    "    x = self.projection_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder = Decoder(InputEmbedding , PositionalEncoding , Masked_Attention , FeedForward , ProjectionLayer , d_model=512 , dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_logits = transformer_decoder(x , transformer_encoder_block(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_enc_2 = 50\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self , EncoderBlock , Decoder):\n",
    "    super().__init__()\n",
    "    self.transformer_encoder = EncoderBlock(n_enc_2 , Encoder , InputEmbedding , PositionalEncoding , NoMaskAttention , FeedForward , d_model=512 , dropout=0.1)\n",
    "    self.transformer_decoder = Decoder(InputEmbedding , PositionalEncoding , Masked_Attention , FeedForward , ProjectionLayer , d_model=512 , dropout=0.1)\n",
    "\n",
    "  def forward(self , input_sen , target_sen):\n",
    "    encoder_output = self.transformer_encoder(input_sen)\n",
    "    logits =  self.transformer_decoder(input_sen , encoder_output)\n",
    "    b , t , c = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(b*t , c) , target_sen.view(b*t))\n",
    "    return logits , loss\n",
    "  \n",
    "transformer = Transformer(EncoderBlock , Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = torch.randint(1 , 25 , (batch_size , seq_len) , dtype=torch.long)\n",
    "target = torch.randint(1,25 , (batch_size , seq_len) , dtype=torch.long)\n",
    "transformer(sample_sentence , target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LET'S TRAIN THE MODEL NOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 100\n",
    "eval_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "  out = {}\n",
    "  for split in ['train' , 'val']:\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "      x , y = get_batch(split)\n",
    "      logits ,loss = transformer(x,y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LET'S GENERATE FROM THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = \"        \"\n",
    "encoded = encode(sample_test)\n",
    "encoded = torch.tensor(encoded , dtype=torch.long)\n",
    "encoded = encoded.view(1 , seq_len)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded\n",
    "memory = encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in transformer.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits ,loss = transformer(xb , yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits ,loss = transformer(xb , yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits ,loss = transformer(xb , yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    logits , loss = transformer(input_ids , input_ids)\n",
    "    tr_dict = []\n",
    "\n",
    "\n",
    "    #next_token = torch.argmax(logits[:, -1, :])\n",
    "    logits = logits[: , -1 , :]\n",
    "    probs = F.softmax(logits , dim=-1)\n",
    "    next_token = torch.multinomial(probs , num_samples=1)\n",
    "\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    memory = torch.cat([memory, next_token], dim=-1)\n",
    "    for i in range(input_ids.shape[1]-1):\n",
    "        tr_dict.append(input_ids[0][i+1].item())\n",
    "  \n",
    "    input_ids = torch.tensor(tr_dict , dtype=torch.long).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = memory[0].tolist()\n",
    "decode(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in transformer.state_dict():\n",
    "    print(param_tensor, \"\\t\", transformer.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in transformer.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
